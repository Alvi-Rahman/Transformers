{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MS_DATA_TRAINER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLpcBtnJWoNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da02bc5-88fd-4668-b0c5-1120146b9e4c"
      },
      "source": [
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ0T_fSmWwGV"
      },
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSMJHdL2mnYk"
      },
      "source": [
        "device = torch_device\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5LD-uk0Khj2"
      },
      "source": [
        "train_encodings_1 = tokenizer.batch_encode_plus(pd.read_csv('data/ms_all.csv')[['#1 String']].values[:,0], truncation=True, padding=True)\n",
        "train_encodings_2 = tokenizer.batch_encode_plus(pd.read_csv('data/ms_all.csv')[['#2 String']].values[:,0], truncation=True, padding=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6QsH4jFKq6F"
      },
      "source": [
        "import torch\n",
        "\n",
        "class MSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings_1, encodings_2):\n",
        "        self.inputs = train_encodings_1\n",
        "        self.targets = train_encodings_2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
        "        target = {'decoder_'+str(key): torch.tensor(val[idx]) for key, val in self.targets.items()}\n",
        "        input[\"labels\"] = target[\"decoder_input_ids\"]\n",
        "        return {**input, **target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOQYTJzQKsxe"
      },
      "source": [
        "train_dataset = MSDataset(train_encodings_1, train_encodings_2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwg4QMx-KuT-"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ydCkULOW7pq"
      },
      "source": [
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAplSqTeKxKm"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OblXoOlZKyjX"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=train_dataset            # evaluation dataset\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "NrQEgqnfK0p3",
        "outputId": "41ca639b-bebd-4ff4-e9be-1b3559fceeec"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='292' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [292/732 06:37 < 10:03, 0.73 it/s, Epoch 1.19/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWYvFlvSK2gn"
      },
      "source": [
        "def get_response(input_text):\n",
        "    encoding = tokenizer.prepare_seq2seq_batch(input_text, return_tensors=\"pt\")\n",
        "    input_ids, attention_masks = encoding[\"input_ids\"].to(torch_device), encoding[\"attention_mask\"].to(torch_device)\n",
        "    translated = model.generate(input_ids=input_ids, \n",
        "                                attention_mask=attention_masks,\n",
        "                                do_sample=True,\n",
        "                                min_length= len(input_text[0].split()) - 2,\n",
        "                                top_k=120,\n",
        "                                top_p=0.95,\n",
        "                                temperature=0.98,\n",
        "                                early_stopping=True,\n",
        "                                num_return_sequences=1,\n",
        "                                no_repeat_ngram_size = 3\n",
        "                               )\n",
        "    \n",
        "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    return tgt_text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ho0obJEWkV8",
        "outputId": "481d95af-355c-4d46-86fe-55c6d0155f13"
      },
      "source": [
        "sentence = \"While itâ€™s easy to get starstruck by its Pro sibling, the OnePlus 9 is a capable alternative at a lower cost. Youâ€™re looking at quite a gap, especially in the US where (for some unknown reason), the base 8/128 GB version of the 9 Pro is not available. This means you can have the vanilla phone for $730 or the 12/256 GB Pro for $1,070\"\n",
        "context = sentence.split('.')\n",
        "if len(context[-1]) < 5:\n",
        "    context = context[:-1]\n",
        "target = get_response(context)\n",
        "\n",
        "print(sentence)\n",
        "print()\n",
        "print(' '.join(target))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3226: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "While itâ€™s easy to get starstruck by its Pro sibling, the OnePlus 9 is a capable alternative at a lower cost. Youâ€™re looking at quite a gap, especially in the US where (for some unknown reason), the base 8/128 GB version of the 9 Pro is not available. This means you can have the vanilla phone for $730 or the 12/256 GB Pro for $1,070\n",
            "\n",
            "While While While it it is it is is is it it it has it it''sss it it its its its it's it itss its itsss itss itssitsitsits its its's itsits itsitsitsssinsinsins ))â€â€ ()))),,)â€)â€)â€)â€â€)â€),â€)â€),â€),â€),â€)â€)););â€((();););;;;););))â€)â€);;);;((â€â€));â€â€)â€)()()()(( It It ItItItIt ItIt It It it it is is is that that that it it that it is it it it a it it the the the it the a a it a a the the an a a a be a a an an an a an the the a an a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCb2Bdv2Wmu6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hPCfxOlKih_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}