{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij8u9Oc5-To8"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Embedding, Dropout\r\n",
        "from tensorflow.keras.models import Sequential, Model\r\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\r\n",
        "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfytDhU4E0Vv"
      },
      "source": [
        "class DataHandler(object):\r\n",
        "    def __init__(self, word_max_length = 30, batch_size = 64, buffer_size = 20000):\r\n",
        "        \r\n",
        "        train_data, test_data = self._load_data()\r\n",
        "        \r\n",
        "        self.tokenizer_ru = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((ru.numpy() for ru, en in train_data), target_vocab_size=2**13)\r\n",
        "        self.tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() for ru, en in train_data), target_vocab_size=2**13)\r\n",
        "        \r\n",
        "        self.train_data = self._prepare_training_data(train_data, word_max_length, batch_size, buffer_size)\r\n",
        "        self.test_data = self._prepare_testing_data(test_data, word_max_length, batch_size)\r\n",
        "        \r\n",
        "    def _load_data(self):\r\n",
        "        data, info = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True, as_supervised=True)\r\n",
        "        return data['train'], data['validation']\r\n",
        "    \r\n",
        "    def _prepare_training_data(self, data, word_max_length, batch_size, buffer_size):\r\n",
        "        data = data.map(self._encode_tf_wrapper)\r\n",
        "        data.filter(lambda x, y: tf.logical_and(tf.size(x) <= word_max_length, tf.size(y) <= word_max_length))\r\n",
        "        data = data.cache()\r\n",
        "        data = data.shuffle(buffer_size).padded_batch(batch_size, padded_shapes=([-1], [-1]))\r\n",
        "        data = data.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "        return data\r\n",
        "        \r\n",
        "    def _prepare_testing_data(self, data, word_max_length, batch_size):\r\n",
        "        data = data.map(self._encode_tf_wrapper)\r\n",
        "        data = data.filter(lambda x, y: tf.logical_and(tf.size(x) <= word_max_length, tf.size(y) <= word_max_length)).padded_batch(batch_size, padded_shapes=([-1], [-1]))\r\n",
        "        \r\n",
        "    \r\n",
        "    def _encode(self, english, russian):\r\n",
        "        russian = [self.tokenizer_ru.vocab_size] + self.tokenizer_ru.encode(russian.numpy()) + [self.tokenizer_ru.vocab_size+1]\r\n",
        "        english = [self.tokenizer_en.vocab_size] + self.tokenizer_en.encode(english.numpy()) + [self.tokenizer_en.vocab_size+1]\r\n",
        "\r\n",
        "        return russian, english\r\n",
        "    \r\n",
        "    def _encode_tf_wrapper(self, pt, en):\r\n",
        "        return tf.py_function(self._encode, [pt, en], [tf.int64, tf.int64])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkNb2UoO6O9"
      },
      "source": [
        "class PositionalEncoding(object):\r\n",
        "    def __init__(self, position, d):\r\n",
        "        angle_rads = self._get_angles(np.arange(position)[:, np.newaxis], np.arange(d)[np.newaxis, :], d)\r\n",
        "\r\n",
        "        sines = np.sin(angle_rads[:, 0::2])\r\n",
        "        cosines = np.cos(angle_rads[:, 1::2])\r\n",
        "        self._encoding = np.concatenate([sines, cosines], axis= -1)\r\n",
        "        self._encoding = self._encoding[np.newaxis,]\r\n",
        "    \r\n",
        "    def _get_angles(self, position, i, d):\r\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d))\r\n",
        "        return position * angle_rates\r\n",
        "    \r\n",
        "    def get_positional_encoding(self):\r\n",
        "        return tf.cast(self._encoding, dtype=tf.float32)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE-WN-o3PFJs"
      },
      "source": [
        "class MaskHandler(object):\r\n",
        "    def padding_mask(self, sequence):\r\n",
        "        sequence = tf.cast(tf.math.equal(sequence, 0), tf.float32)\r\n",
        "        return sequence[:, tf.newaxis, tf.newaxis, :]\r\n",
        "\r\n",
        "    def look_ahead_mask(self, size):\r\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n",
        "        return mask"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk10lYODNl0l"
      },
      "source": [
        "class ScaledDotProductAttentionLayer():\r\n",
        "    def calculate_output_weights(self, q, k, v, mask):\r\n",
        "        qk = tf.matmul(q, k, transpose_b=True)\r\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n",
        "        scaled_attention = qk / tf.math.sqrt(dk)\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            scaled_attention_logits += (mask * -1e9)  \r\n",
        "\r\n",
        "        weights = tf.nn.softmax(scaled_attention, axis=-1)\r\n",
        "        output = tf.matmul(weights, v)\r\n",
        "\r\n",
        "        return output, weights"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh6c8wFXPX9O"
      },
      "source": [
        "class MultiHeadAttentionLayer(Layer):\r\n",
        "    def __init__(self, num_neurons, num_heads):\r\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\r\n",
        "        \r\n",
        "        self.num_heads = num_heads\r\n",
        "        self.num_neurons = num_neurons\r\n",
        "        self.depth = num_neurons // self.num_heads\r\n",
        "        self.attention_layer = ScaledDotProductAttentionLayer()\r\n",
        "        \r\n",
        "        self.q_layer = Dense(num_neurons)\r\n",
        "        self.k_layer = Dense(num_neurons)\r\n",
        "        self.v_layer = Dense(num_neurons)\r\n",
        "\r\n",
        "        self.linear_layer = Dense(num_neurons)\r\n",
        "\r\n",
        "    def split(self, x, batch_size):\r\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n",
        "\r\n",
        "    def call(self, v, k, q, mask):\r\n",
        "        batch_size = tf.shape(q)[0]\r\n",
        "\r\n",
        "        # Run through linear layers\r\n",
        "        q = self.q_layer(q)\r\n",
        "        k = self.k_layer(k)\r\n",
        "        v = self.v_layer(v)\r\n",
        "\r\n",
        "        # Split the heads\r\n",
        "        q = self.split(q, batch_size)\r\n",
        "        k = self.split(k, batch_size)\r\n",
        "        v = self.split(v, batch_size)\r\n",
        "\r\n",
        "        # Run through attention\r\n",
        "        attention_output, weights = self.attention_layer.calculate_output_weights(q, k, v, mask)\r\n",
        "        \r\n",
        "        # Prepare for the rest of processing\r\n",
        "        output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\r\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.num_neurons))\r\n",
        "        \r\n",
        "        # Run through final linear layer\r\n",
        "        output = self.linear_layer(concat_attention)\r\n",
        "\r\n",
        "        return output, weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muAZR5UgPeBt"
      },
      "source": [
        "class PreProcessingLayer(Layer):\r\n",
        "    def __init__(self, num_neurons, vocabular_size):\r\n",
        "        super(PreProcessingLayer, self).__init__()\r\n",
        "        \r\n",
        "        # Initialize\r\n",
        "        self.num_neurons = num_neurons\r\n",
        "\r\n",
        "        # Add embedings and positional encoding\r\n",
        "        self.embedding = Embedding(vocabular_size, self.num_neurons)\r\n",
        "        positional_encoding_handler = PositionalEncoding(vocabular_size, self.num_neurons)\r\n",
        "        self.positional_encoding = positional_encoding_handler.get_positional_encoding()\r\n",
        "\r\n",
        "        # Add embedings and positional encoding\r\n",
        "        self.dropout = Dropout(0.1)\r\n",
        "    \r\n",
        "    def call(self, sequence, training, mask):\r\n",
        "        sequence_lenght = tf.shape(sequence)[1]\r\n",
        "        sequence = self.embedding(sequence)\r\n",
        "\r\n",
        "        sequence *= tf.math.sqrt(tf.cast(self.num_neurons, tf.float32))\r\n",
        "        sequence += self.positional_encoding[:, :sequence_lenght, :]\r\n",
        "        sequence = self.dropout(sequence, training=training)\r\n",
        "        \r\n",
        "        return sequence"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeAWVPSCPk_1"
      },
      "source": [
        "def build_multi_head_attention_layers(num_neurons, num_heads):\r\n",
        "    multi_head_attention_layer = MultiHeadAttentionLayer(num_neurons, num_heads)   \r\n",
        "    dropout = tf.keras.layers.Dropout(0.1)\r\n",
        "    normalization = LayerNormalization(epsilon=1e-6)\r\n",
        "    return multi_head_attention_layer, dropout, normalization\r\n",
        "\r\n",
        "def build_feed_forward_layers(num_neurons, num_hidden_neurons):\r\n",
        "    feed_forward_layer = tf.keras.Sequential()\r\n",
        "    feed_forward_layer.add(Dense(num_hidden_neurons, activation='relu'))\r\n",
        "    feed_forward_layer.add(Dense(num_neurons))\r\n",
        "        \r\n",
        "    dropout = Dropout(0.1)\r\n",
        "    normalization = LayerNormalization(epsilon=1e-6)\r\n",
        "    return feed_forward_layer, dropout, normalization"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw03c8eNPnl7"
      },
      "source": [
        "class EncoderLayer(Layer):\r\n",
        "    def __init__(self, num_neurons, num_hidden_neurons, num_heads):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "\r\n",
        "        # Build multi head attention layer and necessary additional layers\r\n",
        "        self.multi_head_attention_layer, self.attention_dropout, self.attention_normalization = \\\r\n",
        "        build_multi_head_attention_layers(num_neurons, num_heads)   \r\n",
        "            \r\n",
        "        # Build feed-forward neural network and necessary additional layers\r\n",
        "        self.feed_forward_layer, self.feed_forward_dropout, self.feed_forward_normalization = \\\r\n",
        "        build_feed_forward_layers(num_neurons, num_hidden_neurons)\r\n",
        "       \r\n",
        "    def call(self, sequence, training, mask):\r\n",
        "\r\n",
        "        # Calculate attention output\r\n",
        "        attnention_output, _ = self.multi_head_attention_layer(sequence, sequence, sequence, mask)\r\n",
        "        attnention_output = self.attention_dropout(attnention_output, training=training)\r\n",
        "        attnention_output = self.attention_normalization(sequence + attnention_output)\r\n",
        "        \r\n",
        "        # Calculate output of feed forward network\r\n",
        "        output = self.feed_forward_layer(attnention_output)\r\n",
        "        output = self.feed_forward_dropout(output, training=training)\r\n",
        "        \r\n",
        "        # Combine two outputs\r\n",
        "        output = self.feed_forward_normalization(attnention_output + output)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2HnqX4NPrqs"
      },
      "source": [
        "class DecoderLayer(Layer):\r\n",
        "    def __init__(self, num_neurons, num_hidden_neurons, num_heads):\r\n",
        "        super(DecoderLayer, self).__init__()\r\n",
        "\r\n",
        "        # Build multi head attention layers and necessary additional layers\r\n",
        "        self.multi_head_attention_layer1, self.attention_dropout1, self.attention_normalization1 =\\\r\n",
        "        build_multi_head_attention_layers(num_neurons, num_heads)   \r\n",
        "        \r\n",
        "        self.multi_head_attention_layer2, self.attention_dropout2, self.attention_normalization2 =\\\r\n",
        "        build_multi_head_attention_layers(num_neurons, num_heads)           \r\n",
        "\r\n",
        "        # Build feed-forward neural network and necessary additional layers\r\n",
        "        self.feed_forward_layer, self.feed_forward_dropout, self.feed_forward_normalization = \\\r\n",
        "        build_feed_forward_layers(num_neurons, num_hidden_neurons)\r\n",
        "\r\n",
        "    def call(self, sequence, enconder_output, training, look_ahead_mask, padding_mask):\r\n",
        "\r\n",
        "        attnention_output1, attnention_weights1 = self.multi_head_attention_layer1(sequence, sequence, sequence, look_ahead_mask)\r\n",
        "        attnention_output1 = self.attention_dropout1(attnention_output1, training=training)\r\n",
        "        attnention_output1 = self.attention_normalization1(sequence + attnention_output1)\r\n",
        "        \r\n",
        "        attnention_output2, attnention_weights2 = self.multi_head_attention_layer2(enconder_output, enconder_output, attnention_output1, padding_mask)\r\n",
        "        attnention_output2 = self.attention_dropout1(attnention_output2, training=training)\r\n",
        "        attnention_output2 = self.attention_normalization1(attnention_output1 + attnention_output2)\r\n",
        "\r\n",
        "        output = self.feed_forward_layer(attnention_output2)\r\n",
        "        output = self.feed_forward_dropout(output, training=training)\r\n",
        "        output = self.feed_forward_normalization(attnention_output2 + output)\r\n",
        "\r\n",
        "        return output, attnention_weights1, attnention_weights2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLyT8JKMPukk"
      },
      "source": [
        "class Encoder(Layer):\r\n",
        "    def __init__(self, num_neurons, num_hidden_neurons, num_heads, vocabular_size, num_enc_layers = 6):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        \r\n",
        "        self.num_enc_layers = num_enc_layers\r\n",
        "        \r\n",
        "        self.pre_processing_layer = PreProcessingLayer(num_neurons, vocabular_size)\r\n",
        "        self.encoder_layers = [EncoderLayer(num_neurons, num_hidden_neurons, num_heads) for _ in range(num_enc_layers)]\r\n",
        "\r\n",
        "    def call(self, sequence, training, mask):\r\n",
        "        \r\n",
        "        sequence = self.pre_processing_layer(sequence, training, mask)\r\n",
        "        for i in range(self.num_enc_layers):\r\n",
        "            sequence = self.encoder_layers[i](sequence, training, mask)\r\n",
        "\r\n",
        "        return sequence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoRbA_TAPw6E"
      },
      "source": [
        "class Decoder(Layer):\r\n",
        "    def __init__(self, num_neurons, num_hidden_neurons, num_heads, vocabular_size, num_dec_layers=6):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "\r\n",
        "        self.num_dec_layers = num_dec_layers\r\n",
        "        \r\n",
        "        self.pre_processing_layer = PreProcessingLayer(num_neurons, vocabular_size)\r\n",
        "        self.decoder_layers = [DecoderLayer(num_neurons, num_hidden_neurons, num_heads) for _ in range(num_dec_layers)]\r\n",
        "\r\n",
        "    def call(self, sequence, enconder_output, training, look_ahead_mask, padding_mask):\r\n",
        "            \r\n",
        "        sequence = self.pre_processing_layer(sequence, training, mask)\r\n",
        "        \r\n",
        "        for i in range(self.num_dec_layers):\r\n",
        "\r\n",
        "            sequence, attention_weights1, attention_weights2 = self.dec_layers[i](sequence, enconder_output, training, look_ahead_mask, padding_mask)\r\n",
        "\r\n",
        "            attention_weights['decoder_layer{}_attention_weights1'.format(i+1)] = attention_weights1\r\n",
        "            attention_weights['decoder_layer{}_attention_weights2'.format(i+1)] = attention_weights2\r\n",
        "\r\n",
        "        return sequence, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W0uuYKNPy_U"
      },
      "source": [
        "class Transformer(Model):\r\n",
        "    def __init__(self, num_layers, num_neurons, num_hidden_neurons, num_heads, input_vocabular_size, target_vocabular_size):\r\n",
        "        super(Transformer, self).__init__()\r\n",
        "        self.encoder = Encoder(num_neurons, num_hidden_neurons, num_heads, input_vocabular_size, num_layers)\r\n",
        "        self.decoder = Decoder(num_neurons, num_hidden_neurons, num_heads, target_vocabular_size, num_layers)\r\n",
        "        self.linear_layer = Dense(target_vocabular_size)\r\n",
        "\r\n",
        "    def call(self, transformer_input, tar, training, encoder_padding_mask, look_ahead_mask, decoder_padding_mask):\r\n",
        "        encoder_output = self.encoder(transformer_input, training, encoder_padding_mask)\r\n",
        "        decoder_output, attention_weights = self.decoder(tar, encoder_output, training, look_ahead_mask, decoder_padding_mask)\r\n",
        "        output = self.linear_layer(decoder_output)\r\n",
        "\r\n",
        "        return output, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sof_D6izP410"
      },
      "source": [
        "class Schedule(LearningRateSchedule):\r\n",
        "    def __init__(self, num_neurons, warmup_steps=4000):\r\n",
        "        super(Schedule, self).__init__()\r\n",
        "\r\n",
        "        self.num_neurons = tf.cast(num_neurons, tf.float32)\r\n",
        "        self.warmup_steps = warmup_steps\r\n",
        "\r\n",
        "    def __call__(self, step):\r\n",
        "        arg1 = tf.math.rsqrt(step)\r\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\r\n",
        "\r\n",
        "        return tf.math.rsqrt(self.num_neurons) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD0-ynRPP8ZU"
      },
      "source": [
        "loss_objective_function = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "\r\n",
        "def padded_loss_function(real, prediction):\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "    loss = loss_objective_function(real, prediction)\r\n",
        "\r\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\r\n",
        "    loss *= mask\r\n",
        "\r\n",
        "    return tf.reduce_mean(loss)\r\n",
        "\r\n",
        "training_loss = Mean(name='training_loss')\r\n",
        "training_accuracy = SparseCategoricalAccuracy(name='training_accuracy')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sN74ol-S9pb"
      },
      "source": [
        "data_container = DataHandler()\r\n",
        "maskHandler = MaskHandler()\r\n",
        "\r\n",
        "# Initialize parameters\r\n",
        "num_layers = 4\r\n",
        "num_neurons = 128\r\n",
        "num_hidden_layers = 512\r\n",
        "num_heads = 8\r\n",
        "\r\n",
        "# Initialize vocabular size\r\n",
        "input_vocablar_size = data_container.tokenizer_ru.vocab_size + 2\r\n",
        "target_vocablar_size = data_container.tokenizer_en.vocab_size + 2\r\n",
        "\r\n",
        "# Initialize learning rate\r\n",
        "learning_rate = Schedule(num_neurons)\r\n",
        "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGcjoFTCQDea"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Initialize transformer\r\n",
        "transformer = Transformer(num_layers, num_neurons, num_hidden_layers, num_heads, input_vocablar_size, target_vocablar_size)\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF_dYaixQMll"
      },
      "source": [
        "train_step_signature = [\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "]\r\n",
        "\r\n",
        "@tf.function(input_signature=train_step_signature)\r\n",
        "def train_step(input_language, target_language):\r\n",
        "    target_input = target_language[:, :-1]\r\n",
        "    tartet_output = target_language[:, 1:]\r\n",
        "    \r\n",
        "    # Create masks\r\n",
        "    encoder_padding_mask = maskHandler.padding_mask(input_language)\r\n",
        "    decoder_padding_mask = maskHandler.padding_mask(input_language)\r\n",
        "    \r\n",
        "    look_ahead_mask = maskHandler.look_ahead_mask(tf.shape(target_language)[1])\r\n",
        "    decoder_target_padding_mask = maskHandler.padding_mask(target_language)\r\n",
        "    combined_mask = tf.maximum(decoder_target_padding_mask, look_ahead_mask)\r\n",
        "    \r\n",
        "    # Run training step\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        predictions, _ = transformer(input_language, target_input,  True, encoder_padding_mask, combined_mask, decoder_padding_mask)\r\n",
        "        total_loss = padded_loss_function(tartet_output, predictions)\r\n",
        "\r\n",
        "\r\n",
        "    gradients = tape.gradient(total_loss, transformer.trainable_variables)    \r\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "    training_loss(total_loss)\r\n",
        "    training_accuracy(tartet_output, predictions)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "ppsj7n8SP-9y",
        "outputId": "0ef13dbc-3171-4822-f960-e129690d274d"
      },
      "source": [
        "scaled_attention_logits = 0\r\n",
        "for epoch in tqdm(range(20)):\r\n",
        "    training_loss.reset_states()\r\n",
        "    training_accuracy.reset_states()\r\n",
        "\r\n",
        "    for (batch, (input_language, target_language)) in enumerate(data_container.train_data):\r\n",
        "        train_step(input_language, target_language)\r\n",
        "    \r\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch, train_loss.result(), train_accuracy.result()))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ac0935891950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {} Loss {:.4f} Accuracy {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn-XTw84Wt-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRA9PGdHPzC0"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eRUMtunPzFs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlgKoAlPnp0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n23SijMAPeJE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}